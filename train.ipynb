{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, auc\n",
    "\n",
    "#%%\n",
    "seed = 12\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "#%%\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "Input_BASE = \"tc-hard/embeddings/few-shot/\"\n",
    "EMEBEDS_BASE = \"tc-hard/embeddings/few-shot/\"\n",
    "RESULTS_BASE = \"tc-hard/reproduce/results/\"\n",
    "DATA_BASE = f\"tc-hard/dataset/few_shot_split/pep+cdr3b/\"\n",
    "\n",
    "# %%\n",
    "def embed_norm(embeddings):\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# %%\n",
    "def load_embeddings(path, train_df, test_df, split_id = 0):\n",
    "    tcrb_seq_train = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"train-{split_id}.tcrb.npy\"))\n",
    "    # peptide_seq_train = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"train-{split_id}.peptide.npy\"))\n",
    "    label_seq_train = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"train-{split_id}.label.npy\"))\n",
    "    \n",
    "    tcrb_seq_test = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"test-{split_id}.tcrb.npy\"))\n",
    "    # peptide_seq_test = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"test-{split_id}.peptide.npy\"))\n",
    "    label_seq_test = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"test-{split_id}.label.npy\"))\n",
    "\n",
    "\n",
    "    peptide_seq_train = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"train-{split_id}.peptide.npy\"))\n",
    "    peptide_seq_test = np.load(os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode, f\"test-{split_id}.peptide.npy\"))\n",
    "\n",
    "    peptide_uniq_train = list(train_df[\"peptide\"].unique())\n",
    "    peptide_uniq_test = list(test_df[\"peptide\"].unique())\n",
    "    train_df[\"peptide_embed\"] = train_df[\"peptide\"].apply(lambda x: peptide_seq_train[peptide_uniq_train.index(x)])\n",
    "    test_df[\"peptide_embed\"] = test_df[\"peptide\"].apply(lambda x: peptide_seq_test[peptide_uniq_test.index(x)])\n",
    "\n",
    "    peptide_seq_train = train_df[\"peptide_embed\"].values\n",
    "    peptide_seq_train = np.stack(peptide_seq_train, axis = 0)\n",
    "    \n",
    "    peptide_seq_test = test_df[\"peptide_embed\"].values\n",
    "    peptide_seq_test = np.stack(peptide_seq_test, axis = 0)\n",
    "\n",
    "    tcrb_seq_train = embed_norm(tcrb_seq_train)\n",
    "    peptide_seq_train = embed_norm(peptide_seq_train)\n",
    "    tcrb_seq_test = embed_norm(tcrb_seq_test)\n",
    "    peptide_seq_test = embed_norm(peptide_seq_test)\n",
    "\n",
    "    return tcrb_seq_train, peptide_seq_train, label_seq_train, tcrb_seq_test, peptide_seq_test, label_seq_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# train data \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tcrb_seq, peptide_seq, label_seq) -> None:\n",
    "        super().__init__()\n",
    "        # self.tcralpha_seq = torch.from_numpy(np.load(EMEBEDS_BASE + \"tcra.npy\"))\n",
    "        # self.tcrbeta_seq = torch.from_numpy(np.load(EMEBEDS_BASE + \"tcrb.npy\"))\n",
    "        # if mode == \"train\":\n",
    "        self.tcrbeta_seq = torch.from_numpy(tcrb_seq)\n",
    "        self.peptide_seq = torch.from_numpy(peptide_seq)\n",
    "        self.label_seq = torch.from_numpy(label_seq).reshape([-1, 1])\n",
    "\n",
    "        # self.mhc_seq = torch.from_numpy(np.load(EMEBEDS_BASE + \"mhc.npy\"))\n",
    "        # self.peptide_seq = torch.from_numpy(np.load(EMEBEDS_BASE + \"peptide.npy\"))\n",
    "        # self.label_seq = torch.from_numpy(np.load(EMEBEDS_BASE + \"labels.npy\")).reshape([-1, 1])\n",
    "        self.label_seq = self.label_seq.to(dtype = torch.float)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.label_seq.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return self.tcralpha_seq[index], self.tcrbeta_seq[index], self.mhc_seq[index], self.peptide_seq[index], self.label_seq[index]\n",
    "        return self.tcrbeta_seq[index], self.peptide_seq[index], self.label_seq[index]\n",
    "\n",
    "#%%\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--split_id\", type = int, default = 0)\n",
    "# parser.add_argument(\"--epochs\", type = int, default = 100)\n",
    "# parser.add_argument(\"--save\", type = bool, default = True)\n",
    "# parser.add_argument(\"--lr\", type = float, default = 0.02)\n",
    "# parser.add_argument(\"--pretrain_name\", type = str, default = \"moleformer\")\n",
    "# parser.add_argument(\"--neg_generate_mode\", type = str, default = \"only-sampled-negs\")\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "from dotmap import DotMap\n",
    "\n",
    "args = DotMap(\n",
    "    {\n",
    "        \"split_id\": 0,\n",
    "        \"epochs\": 20,\n",
    "#         # \"save\": True,\n",
    "#         # \"lr\": 0.02,\n",
    "        \"pretrain_name\": \"moleformer\",\n",
    "        \"neg_generate_mode\": \"only-neg-assays\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_features, num_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Linear(input_features, hidden_features))\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_features))\n",
    "        self.layers.append(nn.Dropout(0.5))\n",
    "        self.layers.append(nn.LeakyReLU(0.1))\n",
    "        for i in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_features, hidden_features))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_features))\n",
    "            self.layers.append(nn.Dropout(0.5))\n",
    "            self.layers.append(nn.LeakyReLU(0.1))\n",
    "        self.layers.append(nn.Linear(hidden_features, output_features))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # if i == 0:\n",
    "            #     print(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs = 100):\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in trange(epochs):\n",
    "        loss_batch = []\n",
    "        acc_batch = []\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # alpha, beta, mhc, peptide, label = batch\n",
    "            beta, peptide, label = batch\n",
    "            \n",
    "            inputs = torch.cat([beta, peptide], dim = 1)\n",
    "            inputs = inputs.to(device = device)\n",
    "            label = label.to(device = device)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pos_ratio = output.round().sum() / len(output)\n",
    "            acc = (output.round() == label).float().mean()\n",
    "\n",
    "            # if i % 50 == 0:\n",
    "            #     print(f\"epoch {epoch} batch {i} loss: {loss.item()} acc: {acc.item()} pos_ratio: {pos_ratio.item()}\")\n",
    "            \n",
    "            loss_batch.append(loss.item())\n",
    "            acc_batch.append(acc.item())\n",
    "            # print(f\"batch : {i} finished\")\n",
    "        loss_list.append(np.round(np.mean(loss_batch), 3))\n",
    "        acc_list.append(np.round(np.mean(acc_batch), 3))\n",
    "\n",
    "\n",
    "        # print(f\"epoch {epoch} finished\")\n",
    "\n",
    "    return loss_list, acc_list, model\n",
    "\n",
    "# %%\n",
    "# use sklearn to calculate auc-roc, with y_true and y_score\n",
    "def calculate_auc(y_true, y_score):\n",
    "    auc_roc = roc_auc_score(y_true, y_score)\n",
    "    return auc_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "metrics = [\n",
    "    'AUROC',\n",
    "    'Accuracy',\n",
    "    'Recall',\n",
    "    'Precision',\n",
    "    'F1 score',\n",
    "    'AUPR'\n",
    "]\n",
    "\n",
    "def pr_auc(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return pr_auc\n",
    "\n",
    "def get_scores(y_true, y_prob, y_pred):\n",
    "    \"\"\"\n",
    "    Compute a df with all classification metrics and respective scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = [\n",
    "        roc_auc_score(y_true, y_prob),\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        recall_score(y_true, y_pred),\n",
    "        precision_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred),\n",
    "        pr_auc(y_true, y_prob)\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data={'score': scores, 'metrics': metrics})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def evaluate(model, test_loader):\n",
    "    loss_batch = []\n",
    "    acc_batch = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            # alpha, beta, mhc, peptide, label = batch\n",
    "            beta, peptide, label = batch\n",
    "            print(\"length of test data: \", len(label))\n",
    "            \n",
    "            inputs = torch.cat([beta, peptide], dim = 1)\n",
    "            inputs = inputs.to(device = device)\n",
    "            label = label.to(device = device)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            pos_ratio = output.round().sum() / len(output)\n",
    "            \n",
    "            acc = (output.round() == label).float().mean()\n",
    "\n",
    "            # auc_roc = calculate_auc(label.cpu().numpy(), output.cpu().numpy())\n",
    "            scores_df = get_scores(label.cpu().numpy(), output.cpu().numpy(), output.round().cpu().numpy())\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"batch {i} loss: {loss.item()} acc: {acc.item()} pos_ratio: {pos_ratio}\")\n",
    "            \n",
    "            loss_batch.append(np.round(np.mean(loss.item()), 3))\n",
    "            acc_batch.append(np.round(np.mean(acc.item()), 3))\n",
    "            # print(f\"batch : {i} finished\")\n",
    "    return loss_batch, acc_batch, scores_df\n",
    "\n",
    "#%%\n",
    "def make_df(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    # map_keys = {\n",
    "    # 'cdr3.beta': 'tcrb',\n",
    "    # 'antigen.epitope': 'peptide',\n",
    "    # \"label\": \"label\"\n",
    "    # }\n",
    "    # df = df.rename(columns={c: map_keys[c] for c in df.columns})\n",
    "\n",
    "    df['tcrb'] = df['tcrb'].str.replace('O','X')\n",
    "    df['peptide'] = df['peptide'].str.replace('O','X')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_id = 0\n",
    "num_few_shot = 5\n",
    "\n",
    "# train_df_path = os.path.join(DATA_BASE, \"train\", args.neg_generate_mode, f\"{num_few_shot}-train-{split_id}.csv\")\n",
    "# test_df_path = os.path.join(DATA_BASE, \"test\", args.neg_generate_mode, f\"{num_few_shot}-test-{split_id}.csv\")\n",
    "DATA_BASE = f\"tc-hard/dataset/new_split/pep+cdr3b/\"\n",
    "\n",
    "train_df_path = os.path.join(DATA_BASE, \"train\", args.neg_generate_mode, f\"train-{split_id}.csv\")\n",
    "test_df_path = os.path.join(DATA_BASE, \"test\", args.neg_generate_mode, f\"test-{split_id}.csv\")\n",
    "\n",
    "train_df = make_df(train_df_path)\n",
    "test_df = make_df(test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((165980, 3), (40480, 3))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206460, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([train_df, test_df], axis = 0)\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(836,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df[\"peptide\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "def amino_acid_to_smiles(sequence):\n",
    "    molecule = Chem.MolFromSequence(sequence)\n",
    "    smiles = Chem.MolToSmiles(molecule)\n",
    "    return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"peptide_smiles\"] = all_df[\"peptide\"].apply(amino_acid_to_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of tcrb: 14.079306335981398\n",
      "mean length of peptide: 9.745989149389652\n",
      "mean length of peptide: 185.40658302654523\n"
     ]
    }
   ],
   "source": [
    "unique_tcrb = all_df[\"tcrb\"]\n",
    "lengths = np.vectorize(len)(unique_tcrb)\n",
    "mean_length = np.mean(lengths)\n",
    "print(f\"mean length of tcrb: {mean_length}\")\n",
    "\n",
    "unique_tcrb = all_df[\"peptide\"]\n",
    "lengths = np.vectorize(len)(unique_tcrb)\n",
    "mean_length = np.mean(lengths)\n",
    "print(f\"mean length of peptide: {mean_length}\")\n",
    "\n",
    "unique_tcrb = all_df[\"peptide_smiles\"]\n",
    "lengths = np.vectorize(len)(unique_tcrb)\n",
    "mean_length = np.mean(lengths)\n",
    "print(f\"mean length of peptide: {mean_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:15<00:00,  3.77s/it]\n",
      " 20%|██        | 1/5 [01:17<05:09, 77.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data:  40480\n",
      "batch 0 loss: 0.7718985676765442 acc: 0.6464179754257202 pos_ratio: 0.6662796139717102\n",
      "train loss: [0.286, 0.253, 0.242, 0.233, 0.227, 0.219, 0.219, 0.217, 0.214, 0.213, 0.209, 0.209, 0.207, 0.207, 0.205, 0.205, 0.203, 0.201, 0.199, 0.199], train acc: [0.896, 0.914, 0.921, 0.926, 0.928, 0.93, 0.932, 0.932, 0.934, 0.935, 0.935, 0.936, 0.937, 0.938, 0.938, 0.938, 0.939, 0.939, 0.94, 0.94]\n",
      "test loss: 0.772, test acc: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:17<00:00,  3.88s/it]\n",
      " 40%|████      | 2/5 [02:38<03:58, 79.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data:  43293\n",
      "batch 0 loss: 0.6834385395050049 acc: 0.6663202047348022 pos_ratio: 0.7066500186920166\n",
      "train loss: [0.286, 0.256, 0.24, 0.232, 0.227, 0.225, 0.22, 0.216, 0.215, 0.213, 0.211, 0.208, 0.21, 0.207, 0.205, 0.205, 0.203, 0.204, 0.202, 0.201], train acc: [0.897, 0.913, 0.921, 0.926, 0.928, 0.929, 0.931, 0.932, 0.934, 0.935, 0.934, 0.936, 0.936, 0.937, 0.938, 0.938, 0.938, 0.938, 0.939, 0.939]\n",
      "test loss: 0.683, test acc: 0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:15<00:00,  3.79s/it]\n",
      " 60%|██████    | 3/5 [03:56<02:37, 78.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data:  40550\n",
      "batch 0 loss: 0.9582415819168091 acc: 0.6410604119300842 pos_ratio: 0.6241676807403564\n",
      "train loss: [0.293, 0.259, 0.248, 0.239, 0.233, 0.229, 0.227, 0.224, 0.22, 0.219, 0.215, 0.217, 0.214, 0.213, 0.211, 0.211, 0.209, 0.208, 0.206, 0.206], train acc: [0.893, 0.908, 0.915, 0.92, 0.922, 0.924, 0.926, 0.927, 0.928, 0.929, 0.931, 0.93, 0.932, 0.932, 0.933, 0.933, 0.934, 0.934, 0.934, 0.935]\n",
      "test loss: 0.958, test acc: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:16<00:00,  3.84s/it]\n",
      " 80%|████████  | 4/5 [05:15<01:18, 78.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data:  40446\n",
      "batch 0 loss: 0.7067746520042419 acc: 0.7022697329521179 pos_ratio: 0.7477377653121948\n",
      "train loss: [0.303, 0.262, 0.245, 0.239, 0.232, 0.227, 0.227, 0.223, 0.219, 0.218, 0.216, 0.214, 0.214, 0.211, 0.21, 0.209, 0.207, 0.205, 0.205, 0.205], train acc: [0.888, 0.91, 0.918, 0.921, 0.924, 0.927, 0.928, 0.929, 0.931, 0.931, 0.933, 0.933, 0.934, 0.935, 0.935, 0.936, 0.936, 0.936, 0.936, 0.937]\n",
      "test loss: 0.707, test acc: 0.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:15<00:00,  3.78s/it]\n",
      "100%|██████████| 5/5 [06:32<00:00, 78.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test data:  40411\n",
      "batch 0 loss: 0.9571189284324646 acc: 0.600678026676178 pos_ratio: 0.5997129678726196\n",
      "train loss: [0.301, 0.261, 0.248, 0.237, 0.232, 0.227, 0.229, 0.222, 0.22, 0.217, 0.215, 0.214, 0.213, 0.21, 0.21, 0.208, 0.209, 0.208, 0.207, 0.205], train acc: [0.892, 0.911, 0.918, 0.923, 0.926, 0.928, 0.929, 0.93, 0.931, 0.932, 0.934, 0.934, 0.934, 0.936, 0.936, 0.936, 0.937, 0.937, 0.937, 0.938]\n",
      "test loss: 0.957, test acc: 0.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "results_df = []\n",
    "\n",
    "load_path = os.path.join(EMEBEDS_BASE, args.pretrain_name, args.neg_generate_mode)\n",
    "\n",
    "num_few_shot = 5\n",
    "\n",
    "for i in trange(5):\n",
    "    split_id = i\n",
    "\n",
    "    train_df_path = os.path.join(DATA_BASE, \"train\", args.neg_generate_mode, f\"{num_few_shot}-train-{split_id}.csv\")\n",
    "    test_df_path = os.path.join(DATA_BASE, \"test\", args.neg_generate_mode, f\"{num_few_shot}-test-{split_id}.csv\")\n",
    "\n",
    "    train_df = make_df(train_df_path)\n",
    "    test_df = make_df(test_df_path)\n",
    "\n",
    "    tcrb_seq_train, peptide_seq_train, label_seq_train, tcrb_seq_test, peptide_seq_test, label_seq_test = load_embeddings(load_path, train_df, test_df, split_id)\n",
    "\n",
    "    train_data = CustomDataset(tcrb_seq_train, peptide_seq_train, label_seq_train)\n",
    "    test_data = CustomDataset(tcrb_seq_test, peptide_seq_test, label_seq_test)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size = 128, shuffle = True, drop_last = True)\n",
    "    test_loader = DataLoader(test_data, batch_size = 128000, shuffle = False, drop_last = False)\n",
    "\n",
    "    embed_size_tcr = tcrb_seq_train.shape[1]\n",
    "    embed_size_peptide = peptide_seq_train.shape[1]\n",
    "\n",
    "    # input: beta + peptide\n",
    "    model = MLP(input_features = embed_size_tcr + embed_size_peptide, output_features = 1, hidden_features = 100, num_layers = 4)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
    "\n",
    "    loss_list, acc_list, model = train(model, train_loader, args.epochs)\n",
    "    loss_test, acc_test, scores_df = evaluate(model, test_loader)\n",
    "\n",
    "    print(f\"train loss: {loss_list}, train acc: {acc_list}\")\n",
    "    print(f\"test loss: {min(loss_test)}, test acc: {max(acc_test)}\")\n",
    "\n",
    "    scores_df[\"experiment\"] = split_id\n",
    "    scores_df.to_csv(os.path.join(RESULTS_BASE, args.pretrain_name + \"_\" + args.neg_generate_mode + f\"_{split_id}.csv\"), index = False)\n",
    "\n",
    "    results_df.append(scores_df)\n",
    "\n",
    "results_df = pd.concat(results_df)\n",
    "results_df.to_csv(os.path.join(RESULTS_BASE, args.pretrain_name + \"_\" + args.neg_generate_mode + \"_summary.csv\"), index = False)\n",
    "\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
