{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/zhiwei/cq5/miniconda3/envs/Protein/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/project/zhiwei/cq5/miniconda3/envs/Protein/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/project/zhiwei/cq5/miniconda3/envs/Protein/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import random\n",
    "import math\n",
    "import statistics \n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve, auc\n",
    "\n",
    "from matplotlib import collections\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, EsmModel, AutoModelForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device: \", device)\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True\n",
    "\n",
    "# %%\n",
    "# pretrain_name = \"esm2_t6_8M_UR50D\"\n",
    "# pretrain_tcr_name = \"facebook/esm2_t12_35M_UR50D\"\n",
    "# pretrain_name = \"esm2_t30_150M_UR50D\"\n",
    "\n",
    "# pretrain_peptide_name = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "\n",
    "#%%\n",
    "# python train.py --split_id 0 --epochs 10 --save False\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--split_id\", type = int, default = 0)\n",
    "# parser.add_argument(\"--pretrain_name\", type = str, default = \"moleformer\")\n",
    "# parser.add_argument(\"--neg_generate_mode\", type = str, default = \"only-sampled-negs\")\n",
    "\n",
    "# parser.add_argument(\"--epochs\", type = int, default = 10)\n",
    "# parser.add_argument(\"--save\", type = bool, default = True)\n",
    "# parser.add_argument(\"--lr\", type = float, default = 0.02)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "from dotmap import DotMap\n",
    "\n",
    "args = DotMap(\n",
    "    {\n",
    "        \"split_id\": 0,\n",
    "        # \"epochs\": 10,\n",
    "        # \"save\": True,\n",
    "        # \"lr\": 0.02,\n",
    "        \"pretrain_tcr_name\": \"facebook/esm2_t12_35M_UR50D\",\n",
    "        # \"pretrain_peptide_name\": \"SMILES_BERT\",\n",
    "        \"pretrain_peptide_name\": \"moleformer\",\n",
    "        \"neg_generate_mode\": \"only-neg-assays\"\n",
    "    }\n",
    ")\n",
    "\n",
    "#%%\n",
    "if args.neg_generate_mode == \"only-neg-assays\":\n",
    "    DATA_BASE = \"TEINet-master/data/tcrgen/traingen/genval/1neg/\"\n",
    "elif args.neg_generate_mode == \"only-sampled-negs\":\n",
    "    DATA_BASE = \"TEINet-master/data/tcrgen/traingen/genval/2neg/\"\n",
    "\n",
    "EMEBEDS_BASE = \"tc-hard/embeddings/TEIGen/\"\n",
    "\n",
    "train_df_path = os.path.join(DATA_BASE, args.neg_generate_mode, f\"train-{args.split_id}.csv\")\n",
    "validation_df_path = os.path.join(DATA_BASE, args.neg_generate_mode, f\"validation-{args.split_id}.csv\")\n",
    "test_df_path = os.path.join(\"tc-hard/dataset/new_split/pep+cdr3b/test\", args.neg_generate_mode, f\"test-{args.split_id}.csv\")\n",
    "\n",
    "# %%\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "def amino_acid_to_smiles(sequence):\n",
    "    molecule = Chem.MolFromSequence(sequence)\n",
    "    smiles = Chem.MolToSmiles(molecule)\n",
    "    return smiles\n",
    "\n",
    "# %%\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "validation_df = pd.read_csv(validation_df_path)\n",
    "test_df = pd.read_csv(test_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((161187, 3), (43434, 3), (40480, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.rename(columns = {\n",
    "    \"CDR3.beta\": \"tcrb\",\n",
    "    \"Epitope\": \"peptide\",\n",
    "    \"Label\": \"label\"\n",
    "})\n",
    "\n",
    "validation_df = validation_df.rename(columns = {\n",
    "    \"CDR3.beta\": \"tcrb\",\n",
    "    \"Epitope\": \"peptide\",\n",
    "    \"Label\": \"label\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "tcr_tokenizer = AutoTokenizer.from_pretrained(args.pretrain_tcr_name)\n",
    "tcr_embed_model = EsmModel.from_pretrained(args.pretrain_tcr_name)\n",
    "\n",
    "if args.pretrain_peptide_name == \"moleformer\":\n",
    "    peptide_tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)\n",
    "    peptide_embed_model = AutoModel.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "\n",
    "elif args.pretrain_peptide_name == \"SMILES_BERT\":\n",
    "    peptide_tokenizer = AutoTokenizer.from_pretrained(\"JuIm/SMILES_BERT\")\n",
    "    peptide_embed_model = AutoModelForMaskedLM.from_pretrained(\"JuIm/SMILES_BERT\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tcr_embed_model = tcr_embed_model.cuda()\n",
    "    peptide_embed_model = peptide_embed_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_representations(sequence, tokenizer, embed_model):\n",
    "    sequence_representations = []\n",
    "    for protein in sequence:\n",
    "        inputs = tokenizer(protein, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = embed_model(**inputs, output_hidden_states = True)\n",
    "\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        sequence_representation = last_hidden_states[0].mean(dim = 0)\n",
    "        sequence_representations.append(sequence_representation)\n",
    "\n",
    "    return torch.stack(sequence_representations, dim = 0)\n",
    "\n",
    "\n",
    "def get_embeddings(df, save_path, feature, split_set, batch_size = 1000):\n",
    "    feat_seq = df[feature]\n",
    "\n",
    "    result = None\n",
    "\n",
    "    if feature == \"tcrb\":\n",
    "        tokenizer = tcr_tokenizer\n",
    "        embed_model = tcr_embed_model\n",
    "        for k in trange(feat_seq.shape[0] // batch_size + 1):\n",
    "            embeddings = get_representations(feat_seq[k * batch_size: (k + 1) * batch_size], tokenizer, embed_model)\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "            if k == 0:\n",
    "                result = embeddings\n",
    "            else:\n",
    "                result = np.vstack([result, embeddings])\n",
    "        \n",
    "    elif feature == \"peptide\":\n",
    "        tokenizer = peptide_tokenizer\n",
    "        embed_model = peptide_embed_model\n",
    "        \n",
    "        peptide_uniq = list(feat_seq.unique())\n",
    "\n",
    "        if args.pretrain_peptide_name == \"SMILES_BERT\":\n",
    "            inputs = tokenizer(peptide_uniq, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = embed_model(**inputs, output_hidden_states = True)\n",
    "            embeddings = outputs.hidden_states[0].mean(dim = 0)\n",
    "\n",
    "        else:\n",
    "            embeddings = get_representations(peptide_uniq, tokenizer, embed_model)\n",
    "\n",
    "        embeddings = embeddings.detach().cpu().numpy()\n",
    "        result = embeddings\n",
    "\n",
    "    if split_set == \"train\":\n",
    "        save_path = os.path.join(save_path, f\"train-{args.split_id}.\" + feature + \".npy\")\n",
    "    elif split_set == \"validation\":\n",
    "        save_path = os.path.join(save_path, f\"validation-{args.split_id}.\" + feature + \".npy\")\n",
    "    elif split_set == \"test\":\n",
    "        save_path = os.path.join(save_path, f\"test-{args.split_id}.\" + feature + \".npy\")\n",
    "    else:\n",
    "        RaiseError(\"split_set should be one of train, validation, test\")\n",
    "\n",
    "    np.save(save_path, result)\n",
    "    print(f\"embeddings saved to \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "save_path = os.path.join(EMEBEDS_BASE, args.pretrain_peptide_name, args.neg_generate_mode)\n",
    "\n",
    "def embedding_step():\n",
    "    get_embeddings(df = train_df, save_path = save_path, feature = \"tcrb\", split_set = \"train\")\n",
    "    get_embeddings(df = train_df, save_path = save_path, feature = \"peptide\", split_set = \"train\")\n",
    "    get_embeddings(df = validation_df, save_path = save_path, feature = \"tcrb\", split_set = \"validation\")\n",
    "    get_embeddings(df = validation_df, save_path = save_path, feature = \"peptide\", split_set = \"validation\")\n",
    "    get_embeddings(df = test_df, save_path = save_path, feature = \"tcrb\", split_set = \"test\")\n",
    "    get_embeddings(df = test_df, save_path = save_path, feature = \"peptide\", split_set = \"test\")\n",
    "   \n",
    "    np.save(os.path.join(save_path, f\"train-{args.split_id}.label.npy\"), train_df[\"label\"].values)\n",
    "    np.save(os.path.join(save_path, f\"validation-{args.split_id}.label.npy\"), validation_df[\"label\"].values)\n",
    "    np.save(os.path.join(save_path, f\"test-{args.split_id}.label.npy\"), test_df[\"label\"].values)\n",
    "\n",
    "# %%\n",
    "embedding_step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
