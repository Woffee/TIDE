{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, precision_recall_curve\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = f\"vibtcr/data/result/NAbest\"\n",
    "base_path = 'vibtcr/data/new_split/pep+cdr3b/'\n",
    "embed_base_path = 'vibtcr/data/embedNA/val/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[512, 512, 512, 256, 256, 256], dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size = 768 + 480, output_size = 2, hidden_sizes = [32], dropout = 0.3)\n",
    "# model.load_state_dict(torch.load(os.path.join(model_path, \"best_mol-esm_0.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(neg_generate_mode, split_id):\n",
    "    train_df_path = os.path.join(DATA_BASE, \"train\", neg_generate_mode, f\"train-{split_id}.csv\")\n",
    "\n",
    "    test_df_path = os.path.join(DATA_BASE, \"test\", neg_generate_mode, f\"test-{split_id}.csv\")\n",
    "    \n",
    "    train_df = make_df(train_df_path)\n",
    "    test_df = make_df(test_df_path)\n",
    "    \n",
    "    num_samples = train_df.shape[0]\n",
    "    num_validation = int(num_samples * validation_ratio)\n",
    "\n",
    "    from collections import Counter\n",
    "    peptide_count = Counter(train_df['peptide'])\n",
    "    peptide_count_len = len(peptide_count)\n",
    "    peptide_perm = np.random.RandomState(seed=42).permutation(peptide_count_len)\n",
    "\n",
    "    c = 0\n",
    "    selected_peptide = []\n",
    "    for i in peptide_perm:\n",
    "        selected_peptide.append(peptide_count.most_common()[i][0])\n",
    "        c += peptide_count.most_common()[i][1]\n",
    "        if c > num_validation:\n",
    "            break\n",
    "\n",
    "    new_train_df = train_df[~train_df['peptide'].isin(selected_peptide)]\n",
    "    validation_df = train_df[train_df['peptide'].isin(selected_peptide)]\n",
    "\n",
    "    check_unseen(new_train_df, validation_df)\n",
    "    draw_bar(train_df.shape[0], validation_df.shape[0], test_df.shape[0], split_id)\n",
    "    \n",
    "\n",
    "    return new_train_df, validation_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_index):\n",
    "    print(f\"Processing dataset {dataset_index}...\")\n",
    "    \n",
    "    df_train = pd.read_csv(f'{base_path}/train/only-neg-assays/train-{dataset_index}.csv', low_memory=False)\n",
    "    df_val = pd.read_csv(f'{base_path}/validation/only-neg-assays/validation-{dataset_index}.csv', low_memory=False)\n",
    "    df_test = pd.read_csv(f'{base_path}/test/only-neg-assays/test-{dataset_index}.csv', low_memory=False)\n",
    "    train_pep_mol = np.load(f'{embed_base_path}/train{dataset_index}_pep_mol.npy')\n",
    "    val_pep_mol = np.load(f'{embed_base_path}/val{dataset_index}_pep_mol.npy')\n",
    "    test_pep_mol = np.load(f'{embed_base_path}/test{dataset_index}_pep_mol.npy')\n",
    "    train_cdr3 = np.load(f'{embed_base_path}/train{dataset_index}_CDR3_esm.npy')\n",
    "    val_cdr3 = np.load(f'{embed_base_path}/val{dataset_index}_CDR3_esm.npy')\n",
    "    test_cdr3 = np.load(f'{embed_base_path}/test{dataset_index}_CDR3_esm.npy')\n",
    "\n",
    "    X_train = np.column_stack((train_pep_mol, train_cdr3))\n",
    "    X_val = np.column_stack((val_pep_mol, val_cdr3))\n",
    "    X_test = np.column_stack((test_pep_mol, test_cdr3))\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_val = df_val['label']\n",
    "    y_test = df_test['label']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def get_representations(sequence, tokenizer, embed_model):\n",
    "    sequence_representations = []\n",
    "    for protein in sequence:\n",
    "        inputs = tokenizer(protein, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        outputs = embed_model(**inputs, output_hidden_states = True)\n",
    "\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        sequence_representation = last_hidden_states[0].mean(dim = 0)\n",
    "        sequence_representations.append(sequence_representation)\n",
    "\n",
    "    return torch.stack(sequence_representations, dim = 0)\n",
    "\n",
    "\n",
    "def get_embeddings(df, save_path, feature, split_set, batch_size = 1000):\n",
    "    feat_seq = df[feature]\n",
    "\n",
    "    result = None\n",
    "\n",
    "    if feature == \"tcrb\":\n",
    "        tokenizer = tcr_tokenizer\n",
    "        embed_model = tcr_embed_model\n",
    "        for k in trange(feat_seq.shape[0] // batch_size + 1):\n",
    "            embeddings = get_representations(feat_seq[k * batch_size: (k + 1) * batch_size], tokenizer, embed_model)\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "            if k == 0:\n",
    "                result = embeddings\n",
    "            else:\n",
    "                result = np.vstack([result, embeddings])\n",
    "        \n",
    "    elif feature == \"peptide\":\n",
    "        tokenizer = peptide_tokenizer\n",
    "        embed_model = peptide_embed_model\n",
    "        \n",
    "        peptide_uniq = list(feat_seq.unique())\n",
    "\n",
    "        if args.pretrain_peptide_name == \"SMILES_BERT\":\n",
    "            inputs = tokenizer(peptide_uniq, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = embed_model(**inputs, output_hidden_states = True)\n",
    "            embeddings = outputs.hidden_states[0].mean(dim = 0)\n",
    "\n",
    "        else:\n",
    "            embeddings = get_representations(peptide_uniq, tokenizer, embed_model)\n",
    "\n",
    "        embeddings = embeddings.detach().cpu().numpy()\n",
    "        result = embeddings\n",
    "\n",
    "    if split_set == \"train\":\n",
    "        save_path = os.path.join(save_path, f\"train-{args.split_id}.\" + feature + \".npy\")\n",
    "    elif split_set == \"validation\":\n",
    "        save_path = os.path.join(save_path, f\"validation-{args.split_id}.\" + feature + \".npy\")\n",
    "    elif split_set == \"test\":\n",
    "        save_path = os.path.join(save_path, f\"test-{args.split_id}.\" + feature + \".npy\")\n",
    "    else:\n",
    "        RaiseError(\"split_set should be one of train, validation, test\")\n",
    "\n",
    "    np.save(save_path, result)\n",
    "    print(f\"embeddings saved to \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_ratio(seen_ratio = 0):\n",
    "    \n",
    "    num_rows_to_replace = int(np.ceil(X_test.shape[0] * seen_ratio))\n",
    "    rows_to_replace = np.random.choice(X_test.shape[0], size = num_rows_to_replace, replace=False)\n",
    "    rows_to_place = np.random.choice(X_train.shape[0], size = num_rows_to_replace, replace=False)\n",
    "\n",
    "    X_new_test = X_test.copy()\n",
    "    X_new_test[rows_to_replace, :] = X_new_train[rows_to_place, :]\n",
    "\n",
    "    y_new_test = y_test.copy()\n",
    "    y_new_test[rows_to_replace] = y_new_train[rows_to_place]\n",
    "\n",
    "\n",
    "    return X_new_test, y_new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result():\n",
    "\n",
    "    X_new_test, y_new_test = get_diff_seen_ratio(seen_ratio)\n",
    "    y_new_pred = model(torch.from_numpy(X_new_test).to(device))\n",
    "    test_probabilities = torch.softmax(y_new_pred, dim=1)[:, 1].detach().cpu().numpy()\n",
    "    test_predictions = [1 if prob > 0.5 else 0 for prob in test_probabilities]\n",
    "    test_auc = roc_auc_score(y_new_test, test_probabilities)\n",
    "    precision, recall, _ = precision_recall_curve(y_new_test, test_probabilities)\n",
    "\n",
    "    metrics = {\n",
    "            'AUROC': test_auc,\n",
    "            'Accuracy': accuracy_score(y_new_test, test_predictions),\n",
    "            'Recall': recall_score(y_new_test, test_predictions),\n",
    "            'Precision': precision_score(y_new_test, test_predictions),\n",
    "            'F1 score': f1_score(y_new_test, test_predictions),\n",
    "            'AUPR': auc(recall, precision),\n",
    "        }\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "            'score': list(metrics.values()),\n",
    "            'metrics': list(metrics.keys()),\n",
    "            'experiment': [dataset_index] * len(metrics)\n",
    "        })\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, dataset_index, best_model_path):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data(dataset_index)\n",
    "    model = MLP(input_size = 748 + 500, output_size = 2, hidden_sizes = [32], dropout = 0.3)\n",
    "    model.load_state_dict(torch.load(os.path.join(best_model_path, f\"best_mol-esm_{dataset_index}.pth\")))\n",
    "    model = model.to(\"cuda:0\")\n",
    "    model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     test_probabilities = []\n",
    "        # y_true_test = []\n",
    "        # test_running_loss = 0.0\n",
    "        # for test_inputs, test_targets in test_loader:\n",
    "        #     test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "        #     test_outputs = model(test_inputs.float())\n",
    "        #     test_loss = criterion(test_outputs, test_targets)\n",
    "        #     test_running_loss += test_loss.item() * test_inputs.size(0)\n",
    "        #     test_probabilities.extend(torch.softmax(test_outputs, dim=1)[:, 1].cpu().numpy())\n",
    "            # y_true_test.extend(test_targets.cpu().numpy())\n",
    "\n",
    "    y_pred = model(torch.from_numpy(X_test).to(\"cuda:0\"))\n",
    "    test_probabilities = torch.softmax(y_pred, dim=1)[:, 1].detach().cpu().numpy()\n",
    "    y_true_test = y_test\n",
    "    # test_loss = test_running_loss / len(test_loader.dataset)    \n",
    "    test_auc = roc_auc_score(y_true_test, test_probabilities)    \n",
    "    test_predictions = [1 if prob > 0.5 else 0 for prob in test_probabilities]\n",
    "    precision, recall, _ = precision_recall_curve(y_true_test, test_probabilities)\n",
    "    \n",
    "    metrics = {\n",
    "            'AUROC': test_auc,\n",
    "            'Accuracy': accuracy_score(y_true_test, test_predictions),\n",
    "            'Recall': recall_score(y_true_test, test_predictions),\n",
    "            'Precision': precision_score(y_true_test, test_predictions),\n",
    "            'F1 score': f1_score(y_true_test, test_predictions),\n",
    "            'AUPR': auc(recall, precision),\n",
    "        }\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "            'score': list(metrics.values()),\n",
    "            'metrics': list(metrics.keys()),\n",
    "            'experiment': dataset_index\n",
    "        })\n",
    "    \n",
    "    # csv_path = os.path.join(results_dir, f\"evaluation_{dataset_index}.csv\")\n",
    "    #result_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"\\nBest Model Performance and Evaluation of dataset{dataset_index}:\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"{metric}: {score*100:.4f}%\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 0...\n",
      "\n",
      "Best Model Performance and Evaluation of dataset0:\n",
      "AUROC: 89.2157%\n",
      "Accuracy: 97.6383%\n",
      "Recall: 99.6219%\n",
      "Precision: 98.0015%\n",
      "F1 score: 98.8051%\n",
      "AUPR: 99.7560%\n",
      "Processing dataset 1...\n",
      "\n",
      "Best Model Performance and Evaluation of dataset1:\n",
      "AUROC: 82.4534%\n",
      "Accuracy: 91.4882%\n",
      "Recall: 99.4476%\n",
      "Precision: 91.9553%\n",
      "F1 score: 95.5548%\n",
      "AUPR: 98.1988%\n",
      "Processing dataset 2...\n",
      "\n",
      "Best Model Performance and Evaluation of dataset2:\n",
      "AUROC: 84.8993%\n",
      "Accuracy: 97.5290%\n",
      "Recall: 99.5895%\n",
      "Precision: 97.9226%\n",
      "F1 score: 98.7490%\n",
      "AUPR: 99.6134%\n",
      "Processing dataset 3...\n",
      "\n",
      "Best Model Performance and Evaluation of dataset3:\n",
      "AUROC: 99.1751%\n",
      "Accuracy: 95.2060%\n",
      "Recall: 100.0000%\n",
      "Precision: 95.2060%\n",
      "F1 score: 97.5441%\n",
      "AUPR: 99.9552%\n",
      "Processing dataset 4...\n",
      "\n",
      "Best Model Performance and Evaluation of dataset4:\n",
      "AUROC: 94.7579%\n",
      "Accuracy: 99.8713%\n",
      "Recall: 99.9777%\n",
      "Precision: 99.8936%\n",
      "F1 score: 99.9356%\n",
      "AUPR: 99.9942%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>metrics</th>\n",
       "      <th>experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.892157</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.976383</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996219</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.980015</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988051</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997560</td>\n",
       "      <td>AUPR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.824534</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.914882</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994476</td>\n",
       "      <td>Recall</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.919553</td>\n",
       "      <td>Precision</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.955548</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.981988</td>\n",
       "      <td>AUPR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848993</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.975290</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995895</td>\n",
       "      <td>Recall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.979226</td>\n",
       "      <td>Precision</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.987490</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996134</td>\n",
       "      <td>AUPR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991751</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.952060</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Recall</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952060</td>\n",
       "      <td>Precision</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975441</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999552</td>\n",
       "      <td>AUPR</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.947579</td>\n",
       "      <td>AUROC</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998713</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999777</td>\n",
       "      <td>Recall</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998936</td>\n",
       "      <td>Precision</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999356</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999942</td>\n",
       "      <td>AUPR</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score    metrics  experiment\n",
       "0  0.892157      AUROC           0\n",
       "1  0.976383   Accuracy           0\n",
       "2  0.996219     Recall           0\n",
       "3  0.980015  Precision           0\n",
       "4  0.988051   F1 score           0\n",
       "5  0.997560       AUPR           0\n",
       "0  0.824534      AUROC           1\n",
       "1  0.914882   Accuracy           1\n",
       "2  0.994476     Recall           1\n",
       "3  0.919553  Precision           1\n",
       "4  0.955548   F1 score           1\n",
       "5  0.981988       AUPR           1\n",
       "0  0.848993      AUROC           2\n",
       "1  0.975290   Accuracy           2\n",
       "2  0.995895     Recall           2\n",
       "3  0.979226  Precision           2\n",
       "4  0.987490   F1 score           2\n",
       "5  0.996134       AUPR           2\n",
       "0  0.991751      AUROC           3\n",
       "1  0.952060   Accuracy           3\n",
       "2  1.000000     Recall           3\n",
       "3  0.952060  Precision           3\n",
       "4  0.975441   F1 score           3\n",
       "5  0.999552       AUPR           3\n",
       "0  0.947579      AUROC           4\n",
       "1  0.998713   Accuracy           4\n",
       "2  0.999777     Recall           4\n",
       "3  0.998936  Precision           4\n",
       "4  0.999356   F1 score           4\n",
       "5  0.999942       AUPR           4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    \"score\": [],\n",
    "    \"metrics\": [],\n",
    "    \"experiment\": [] \n",
    "})\n",
    "l = []\n",
    "for dataset_index in range(5):\n",
    "    l.append(evaluate_and_save(model, dataset_index, best_model_path))\n",
    "result_df = pd.concat(l)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUPR</td>\n",
       "      <td>0.995035</td>\n",
       "      <td>0.007455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUROC</td>\n",
       "      <td>0.901003</td>\n",
       "      <td>0.068984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.963466</td>\n",
       "      <td>0.031779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.981177</td>\n",
       "      <td>0.016638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.965958</td>\n",
       "      <td>0.030853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.997274</td>\n",
       "      <td>0.002477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     metrics      mean       std\n",
       "0       AUPR  0.995035  0.007455\n",
       "1      AUROC  0.901003  0.068984\n",
       "2   Accuracy  0.963466  0.031779\n",
       "3   F1 score  0.981177  0.016638\n",
       "4  Precision  0.965958  0.030853\n",
       "5     Recall  0.997274  0.002477"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = result_df.groupby('metrics')['score'].agg(['mean', 'std']).reset_index()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(dataset_index):\n",
    "    print(f\"Processing dataset {dataset_index}...\")\n",
    "    \n",
    "    df_train = pd.read_csv(f'{base_path}/train/only-neg-assays/train-{dataset_index}.csv', low_memory=False)\n",
    "    df_val = pd.read_csv(f'{base_path}/validation/only-neg-assays/validation-{dataset_index}.csv', low_memory=False)\n",
    "    df_test = pd.read_csv(f'{base_path}/test/only-neg-assays/test-{dataset_index}.csv', low_memory=False)\n",
    "\n",
    "    # return df_train, df_val, df_test\n",
    "    train_pep_mol = np.load(f'{embed_base_path}/train{dataset_index}_pep_mol.npy')\n",
    "    val_pep_mol = np.load(f'{embed_base_path}/val{dataset_index}_pep_mol.npy')\n",
    "    test_pep_mol = np.load(f'{embed_base_path}/test{dataset_index}_pep_mol.npy')\n",
    "    \n",
    "\n",
    "    train_cdr3 = np.load(f'{embed_base_path}/train{dataset_index}_CDR3_esm.npy')\n",
    "    val_cdr3 = np.load(f'{embed_base_path}/val{dataset_index}_CDR3_esm.npy')\n",
    "    test_cdr3 = np.load(f'{embed_base_path}/test{dataset_index}_CDR3_esm.npy')\n",
    "\n",
    "    X_train = np.column_stack((train_pep_mol, train_cdr3))\n",
    "    X_val = np.column_stack((val_pep_mol, val_cdr3))\n",
    "    X_test = np.column_stack((test_pep_mol, test_cdr3))\n",
    "\n",
    "    y_train = df_train['label']\n",
    "    y_val = df_val['label']\n",
    "    y_test = df_test['label']\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    train_pep_mol = X_train[:, :768]\n",
    "    train_cdr3 = X_train[:, 768:]\n",
    "    val_pep_mol = X_val[:, :768]\n",
    "    val_cdr3 = X_val[:, 768:]\n",
    "    test_pep_mol = X_test[:, :768]\n",
    "    test_cdr3 = X_test[:, 768:]\n",
    "    \n",
    "\n",
    "    return df_train, df_val, df_test, train_pep_mol, train_cdr3, val_pep_mol, val_cdr3, test_pep_mol, test_cdr3, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 0...\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test, train_pep_mol, train_cdr3, val_pep_mol, val_cdr3, test_pep_mol, test_cdr3, y_train, y_val, y_test = test_dataset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268961, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([df_train, df_val, df_test])\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((268961, 768), (268961, 480))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_peptide_embed = np.vstack([train_pep_mol, val_pep_mol, test_pep_mol])\n",
    "all_tcrb_embed = np.vstack([train_cdr3, val_cdr3, test_cdr3])\n",
    "\n",
    "all_peptide_embed.shape, all_tcrb_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"peptide_embed\"] = list(all_peptide_embed)\n",
    "all_df[\"tcrb_embed\"] = list(all_tcrb_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1360, 5), (197850, 5))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_unique_df = all_df.drop_duplicates(subset='peptide')\n",
    "tcrb_unique_df = all_df.drop_duplicates(subset='tcrb')\n",
    "\n",
    "peptide_unique_df.shape, tcrb_unique_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_embed_dict = dict(zip(peptide_unique_df[\"peptide\"], peptide_unique_df[\"peptide_embed\"]))\n",
    "tcrb_embed_dict = dict(zip(tcrb_unique_df[\"tcrb\"], tcrb_unique_df[\"tcrb_embed\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tc-hard/meta_data/moleformer/only-neg-assays/peptide_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(peptide_embed_dict, f)\n",
    "\n",
    "with open(\"tc-hard/meta_data/moleformer/only-neg-assays/tcrb_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tcrb_embed_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
